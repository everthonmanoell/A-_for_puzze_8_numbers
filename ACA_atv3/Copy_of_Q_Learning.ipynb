{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkJEA8Ek_kbJ"
      },
      "source": [
        "[CEL1] **MATRIZ DE TRANSIÇÃO DE ESTADOS (PARA SIMULAÇÃO DO AGENTE) **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mb5mLKvCyHaw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cria uma matriz 11x11 preenchida com zeros\n",
        "T_up = np.zeros((11, 11))\n",
        "\n",
        "# Preenche a matriz com as probabilidades de transição\n",
        "T_up[0, 3] = 0.1\n",
        "T_up[0, 0] = 0.1\n",
        "T_up[0, 1] = 0.8\n",
        "T_up[1, 1] = 0.2\n",
        "T_up[1, 2] = 0.8\n",
        "T_up[2, 4] = 0.1\n",
        "T_up[2, 2] = 0.9\n",
        "T_up[3, 5] = 0.1\n",
        "T_up[3, 3] = 0.8\n",
        "T_up[3, 0] = 0.1\n",
        "T_up[4, 7] = 0.1\n",
        "T_up[4, 2] = 0.1\n",
        "T_up[4, 4] = 0.8\n",
        "T_up[5, 8] = 0.1\n",
        "T_up[5, 3] = 0.1\n",
        "T_up[5, 6] = 0.8\n",
        "T_up[6, 9] = 0.1\n",
        "T_up[6, 6] = 0.1\n",
        "T_up[6, 7] = 0.8\n",
        "T_up[7, 10] = 0.1\n",
        "T_up[7, 4] = 0.1\n",
        "T_up[7, 7] = 0.8\n",
        "T_up[8, 8] = 0.1\n",
        "T_up[8, 5] = 0.1\n",
        "T_up[8, 9] = 0.8\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# MATRIZ DE TRANSIÇÃO PARA A AÇÃO DOWN\n",
        "T_down = np.zeros((11, 11))\n",
        "T_down[0, 0] = 0.9\n",
        "T_down[0, 3] = 0.1\n",
        "T_down[1, 0] = 0.8\n",
        "T_down[1, 1] = 0.2\n",
        "T_down[2, 1] = 0.8\n",
        "T_down[2, 2] = 0.1\n",
        "T_down[2, 4] = 0.1\n",
        "T_down[3, 0] = 0.1\n",
        "T_down[3, 5] = 0.1\n",
        "T_down[3, 3] = 0.8\n",
        "T_down[4, 4] = 0.8\n",
        "T_down[4, 2] = 0.1\n",
        "T_down[4, 7] = 0.1\n",
        "T_down[5, 5] = 0.8\n",
        "T_down[5, 3] = 0.1\n",
        "T_down[5, 8] = 0.1\n",
        "T_down[6, 5] = 0.8\n",
        "T_down[6, 6] = 0.1\n",
        "T_down[6, 9] = 0.1\n",
        "T_down[7, 6] = 0.8\n",
        "T_down[7, 4] = 0.1\n",
        "T_down[7, 10] = 0.1\n",
        "T_down[8, 8] = 0.9\n",
        "T_down[8, 5] = 0.1\n",
        "\n",
        "# MATRIZ DE TRANSIÇÃO PARA A AÇÃO LEFT\n",
        "T_left = np.zeros((11, 11))\n",
        "T_left[0, 0] = 0.9\n",
        "T_left[0, 1] = 0.1\n",
        "T_left[1, 0] = 0.1\n",
        "T_left[1, 2] = 0.1\n",
        "T_left[1, 1] = 0.8\n",
        "T_left[2, 2] = 0.9\n",
        "T_left[2, 1] = 0.1\n",
        "T_left[3, 0] = 0.8\n",
        "T_left[3, 3] = 0.2\n",
        "T_left[4, 2] = 0.8\n",
        "T_left[4, 4] = 0.2\n",
        "T_left[5, 3] = 0.8\n",
        "T_left[5, 5] = 0.1\n",
        "T_left[5, 6] = 0.1\n",
        "T_left[6, 6] = 0.8\n",
        "T_left[6, 5] = 0.1\n",
        "T_left[6, 7] = 0.1\n",
        "T_left[7, 4] = 0.8\n",
        "T_left[7, 7] = 0.1\n",
        "T_left[7, 6] = 0.1\n",
        "T_left[8, 5] = 0.8\n",
        "T_left[8, 8] = 0.1\n",
        "T_left[8, 9] = 0.1\n",
        "\n",
        "# MATRIZ DE TRANSIÇÃO PARA A AÇÃO RIGHT\n",
        "T_right = np.zeros((11, 11))\n",
        "T_right[0, 3] = 0.8\n",
        "T_right[0, 0] = 0.1\n",
        "T_right[0, 1] = 0.1\n",
        "T_right[1, 1] = 0.8\n",
        "T_right[1, 0] = 0.1\n",
        "T_right[1, 2] = 0.1\n",
        "T_right[2, 4] = 0.8\n",
        "T_right[2, 2] = 0.1\n",
        "T_right[2, 1] = 0.1\n",
        "T_right[3, 5] = 0.8\n",
        "T_right[3, 3] = 0.2\n",
        "T_right[4, 7] = 0.8\n",
        "T_right[4, 4] = 0.2\n",
        "T_right[5, 8] = 0.9\n",
        "T_right[5, 5] = 0.1\n",
        "T_right[5, 6] = 0.1\n",
        "T_right[6, 9] = 0.8\n",
        "T_right[6, 5] = 0.1\n",
        "T_right[6, 7] = 0.1\n",
        "T_right[7, 10] = 0.8\n",
        "T_right[7, 6] = 0.1\n",
        "T_right[7, 7] = 0.1\n",
        "T_right[8, 9] = 0.1\n",
        "T_right[8, 8] = 0.9\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfe8LApI_vkZ"
      },
      "source": [
        "[CEL2] **FUNÇÕES AUXILIARES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDx5op7SzTta"
      },
      "outputs": [],
      "source": [
        "# Função para simular o resultado estocástico de aplicar uma ação ao estado atual\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calc_action_result(state, transition_state):\n",
        "\n",
        "    # Obtém os índices dos estados candidatos (com probabilidade > 0)\n",
        "    cand_states = np.where(transition_state != 0)[0]\n",
        "    prod_cand_states = transition_state[cand_states]\n",
        "\n",
        "    # Ordena as probabilidades e obtém os índices ordenados\n",
        "    sorted_indices = np.argsort(prod_cand_states)\n",
        "    cand_states_sort = cand_states[sorted_indices]\n",
        "    prod_cand_states_sort = prod_cand_states[sorted_indices]\n",
        "\n",
        "    # Constrói a roleta acumulada\n",
        "    roleta = np.cumsum(prod_cand_states_sort)\n",
        "\n",
        "    # Gera número aleatório uniforme entre 0 e 1\n",
        "    r = np.random.uniform()\n",
        "\n",
        "    # Encontra o primeiro índice da roleta onde a probabilidade acumulada excede r\n",
        "    ind = np.where(roleta > r)[0]\n",
        "\n",
        "    return cand_states_sort[ind[0]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[CEL3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OtPWKXcJHKJ_"
      },
      "outputs": [],
      "source": [
        "def choose_best_action(q_matrix, state):\n",
        "    \"\"\"\n",
        "    Função que retorna o índice da melhor ação (com maior valor Q)\n",
        "    para o estado atual.\n",
        "    \"\"\"\n",
        "    act = np.argmax(q_matrix[state])  # retorna índice (0-based)\n",
        "    return act\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIJFfIWIGt5Z"
      },
      "source": [
        "[CEL4] Regra de Aprendizagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fcNs-mjB0tzA"
      },
      "outputs": [],
      "source": [
        "def q_update(state, action, next_state, rw, q_matrix, alpha, gamma):\n",
        "    \"\"\"\n",
        "    Atualiza a estimativa Q para o par (estado, ação) com base na recompensa observada\n",
        "    e na estimativa de valor futuro.\n",
        "\n",
        "    Parâmetros:\n",
        "        state: estado atual (índice inteiro)\n",
        "        action: ação tomada (índice inteiro)\n",
        "        next_state: próximo estado (índice inteiro)\n",
        "        rw: vetor de recompensas por estado\n",
        "        q_matrix: matriz Q (numpy array 2D)\n",
        "        alpha: taxa de aprendizado\n",
        "        gamma: fator de desconto\n",
        "\n",
        "    Retorna:\n",
        "        Novo valor Q para o par (state, action)\n",
        "    \"\"\"\n",
        "    estimate_q = rw[state] + gamma * np.max(q_matrix[next_state, :])\n",
        "    q_value = q_matrix[state, action] + alpha * (estimate_q - q_matrix[state, action])\n",
        "    return q_value\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[CEL5] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VIZRpAPOHafv"
      },
      "outputs": [],
      "source": [
        "# Função para executar a política aprendida e registrar a recompensa total acumulada\n",
        "\n",
        "def simulate_policy(q_matrix, rw):\n",
        "\n",
        "  r_total = 0\n",
        "\n",
        "  state = 0  # em Python usamos índice 0, então 1 em R vira 0 aqui\n",
        "\n",
        "  terminal = True\n",
        "\n",
        "  while terminal:\n",
        "\n",
        "      # Escolher ação com base na política aprendida\n",
        "      action_trial = choose_best_action(q_matrix, state)\n",
        "\n",
        "      # Selecionar a matriz de transição correspondente à ação\n",
        "      if action_trial == 0:\n",
        "          transition_state = T_up[state]\n",
        "      elif action_trial == 1:\n",
        "          transition_state = T_down[state]\n",
        "      elif action_trial == 2:\n",
        "          transition_state = T_left[state]\n",
        "      elif action_trial == 3:\n",
        "          transition_state = T_right[state]\n",
        "\n",
        "      # Aplicar a ação e observar o próximo estado\n",
        "      next_state = calc_action_result(state, transition_state)\n",
        "\n",
        "      print(f\"{state} {actions_names[action_trial]} {next_state}\")\n",
        "\n",
        "      # Acumular recompensa\n",
        "      r_total += rw[next_state]\n",
        "\n",
        "      # Atualizar estado\n",
        "      state = next_state\n",
        "\n",
        "      # Verificar se é estado terminal\n",
        "      if state == 9 or state == 10:  # 10 e 11 em R = 9 e 10 em Python\n",
        "        terminal = False\n",
        "\n",
        "  # Resultado total acumulado\n",
        "  return r_total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[CEL6] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-udm4hd-IVZ"
      },
      "outputs": [],
      "source": [
        "# Função auxiliar para imprimir a política gerada a partir da matrix q\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def print_policy(q_matrix, actions):\n",
        "\n",
        "  # policy: índice da ação com maior valor Q em cada estado (max.col equivalente)\n",
        "  policy = np.argmax(q_matrix, axis=1)  # retorna array com índice da melhor ação por estado\n",
        "\n",
        "  # Em Python, índice começa em 0, então ajustamos os índices usados para extrair elementos do policy:\n",
        "  s1 = \" \".join([actions[policy[2]], actions[policy[4]], actions[policy[7]], \"+1\"])\n",
        "  s2 = \" \".join([actions[policy[1]], \"*\", actions[policy[6]], \"-1\"])\n",
        "  s3 = \" \".join([actions[policy[0]], actions[policy[3]], actions[policy[5]], actions[policy[8]]])\n",
        "\n",
        "  print(\"\\n\", s1, \"\\n\", s2, \"\\n\", s3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNV1iIrr2h3B"
      },
      "source": [
        "[CEL7] **INICIALIZAÇÃO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNO3jjVB2Vna"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Nomes das ações (para referência, se quiser usar como rótulos)\n",
        "actions_names = [\"UP\", \"DW\", \"LF\", \"RG\"]\n",
        "\n",
        "# Inicialização da matriz Q (11 estados x 4 ações)\n",
        "q_matrix = np.zeros((11, 4))\n",
        "\n",
        "# Estados terminais (indexando a partir de 0 → estado 10 e 11 em R são 9 e 10 em Python)\n",
        "q_matrix[9, :] = -1    # Estado 10 (índice 9)\n",
        "q_matrix[10, :] = 1    # Estado 11 (índice 10)\n",
        "\n",
        "# Matriz de contagem de visitas para cada par (estado, ação) – opcional para controle\n",
        "N_matrix = np.zeros((11, 4))\n",
        "\n",
        "# Hiperparâmetros\n",
        "alpha = 0.2\n",
        "gamma = 0.5\n",
        "\n",
        "# Vetor de recompensas por estado\n",
        "rw = np.full(11, -0.04)\n",
        "rw[9] = -1     # Estado 10\n",
        "rw[10] = 1     # Estado 11\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHL836G8_eav"
      },
      "source": [
        "[CEL4] **APRENDIZADO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBLCF1wL2mCx",
        "outputId": "65f56ee2-5c81-44e4-ff50-6f0d58aa34f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          UP        DW        LF        RG\n",
            "0  -0.060391 -0.068421 -0.069910 -0.060474\n",
            "1  -0.041634 -0.068825 -0.058348 -0.059745\n",
            "2  -0.034551 -0.053640 -0.035446 -0.008607\n",
            "3  -0.056761 -0.056037 -0.065614 -0.039973\n",
            "4   0.057765  0.023059 -0.021611  0.113129\n",
            "5   0.007048 -0.040158 -0.056706 -0.065181\n",
            "6   0.150774 -0.035401  0.032340 -0.434310\n",
            "7   0.159518  0.142964  0.029277  0.411862\n",
            "8  -0.493749 -0.067662 -0.088896 -0.121622\n",
            "9  -1.000000 -1.000000 -1.000000 -1.000000\n",
            "10  1.000000  1.000000  1.000000  1.000000\n",
            "\n",
            " RG RG RG +1 \n",
            " UP * UP -1 \n",
            " UP RG UP DW\n"
          ]
        }
      ],
      "source": [
        "# Cada execução do loop representa uma exploração do ambiente\n",
        "# a partir do estado inicial, escolhendo ações aleatórias\n",
        "# até alcançar um estado terminal.\n",
        "import pandas as pd\n",
        "\n",
        "# número de trajetórias\n",
        "\n",
        "T = 200\n",
        "\n",
        "for i in range(T):\n",
        "\n",
        "    state = 0  # estado 1 em R → índice 0 em Python\n",
        "    terminal = True\n",
        "\n",
        "    while terminal:\n",
        "        # Escolher uma ação aleatória\n",
        "        action_trial = np.random.choice([0, 1, 2, 3])  # 0: UP, 1: DW, 2: LF, 3: RG\n",
        "\n",
        "        # Incrementar a contagem de visitas\n",
        "        N_matrix[state, action_trial] += 1\n",
        "\n",
        "        # Selecionar a matriz de transição correspondente à ação escolhida\n",
        "        if action_trial == 0:\n",
        "            transition_state = T_up[state, :]\n",
        "        elif action_trial == 1:\n",
        "            transition_state = T_down[state, :]\n",
        "        elif action_trial == 2:\n",
        "            transition_state = T_left[state, :]\n",
        "        elif action_trial == 3:\n",
        "            transition_state = T_right[state, :]\n",
        "\n",
        "        # Simular a ação e obter o próximo estado\n",
        "        next_state = calc_action_result(state, transition_state)\n",
        "\n",
        "        # Mostrar a transição\n",
        "        #### print(f\"{state} {actions_names[action_trial]} {next_state}\")\n",
        "\n",
        "        # Atualizar o valor Q\n",
        "        q_matrix[state, action_trial] = q_update(\n",
        "            state, action_trial, next_state, rw, q_matrix, alpha, gamma\n",
        "        )\n",
        "\n",
        "        # Atualizar o estado atual\n",
        "        state = next_state\n",
        "\n",
        "        # Verifica se é estado terminal (estado 10 ou 11 → índices 9 ou 10)\n",
        "        if state == 9 or state == 10:\n",
        "            terminal = False\n",
        "\n",
        "#    print(\"\")  # linha em branco entre episódios\n",
        "\n",
        "\n",
        "q_matrix_df = pd.DataFrame(q_matrix, columns=actions_names)\n",
        "\n",
        "print(q_matrix_df)\n",
        "\n",
        "print_policy(q_matrix, actions_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzR_HTeiIS4T"
      },
      "source": [
        "[CEL9] **SIMULANDO A POLÍTICA APRENDIDA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6qhRRu3G9xA",
        "outputId": "b87804f2-764f-4df5-a82b-475be202d232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 UP 1\n",
            "1 UP 1\n",
            "1 UP 1\n",
            "1 UP 2\n",
            "2 RG 4\n",
            "4 RG 7\n",
            "7 RG 10\n",
            "0.76\n"
          ]
        }
      ],
      "source": [
        "# Simulando a execução da política a partir do estado inicial ate o estado final alcançado e registrado a recompensa total obtiga pelo agente\n",
        "\n",
        "r_total = simulate_policy(q_matrix, rw)\n",
        "\n",
        "print(r_total)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4QucP5OL7tT"
      },
      "source": [
        "# **ITEM 1:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3hSXomRKuVY"
      },
      "source": [
        "* Execute o Q Learning variando os parâmetros de alpha e gamma, com cinco opções de valores para cada parâmetro. Obs.: vc pode usar como critério de parada o número de 30 trajetórias.\n",
        "\n",
        "* Execute o Q Learning 10 vezes para cada combinação de alpha e gamma e avalie a política aprendida. Para avaliar uma política, simule a execução da política por várias vezes (por exemplo, 100 vezes) e registre a média da recompensa total recebida.\n",
        "\n",
        "* Defina assim a melhor configuração de alpha e gamma avaliada.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYXYjKonMCFq"
      },
      "source": [
        "# **ITEM 2:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFiQbZzDMFiH"
      },
      "source": [
        "* Implemente duas estratégias de exploração de estados, como eps-greedy, Boltzman ou UCB. Execute e avalie o Q Learning com cada uma das estratégias.\n",
        "\n",
        "* Obs.: para simplificar fixe os valores de alpha e gamma obtidos no item anterior, mas se tiver tempo pode realizar experimentos adicionais. Novamente vc pode usar o número de 30 trajetórias como critério de parada.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
